{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Radhika/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: the sets module is deprecated\n",
      "  \n",
      "/Users/Radhika/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sets import Set\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.cross_validation import KFold\n",
    "from utils import *\n",
    "from tflearn.activations import relu\n",
    "from sklearn.cross_validation import train_test_split,StratifiedKFold\n",
    "import sys\n",
    "from optparse import OptionParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage: ipykernel_launcher.py [options]\n",
      "\n",
      "ipykernel_launcher.py: error: no such option: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "parser = OptionParser()\n",
    "parser.add_option(\"-d\", \"--d\", default=1024, help=\"The embedding dimension d\")\n",
    "parser.add_option(\"-n\",\"--n\",default=1, help=\"global norm to be clipped\")\n",
    "parser.add_option(\"-k\",\"--k\",default=512,help=\"The dimension of project matrices k\")\n",
    "parser.add_option(\"-t\",\"--t\",default = \"o\",help=\"Test scenario\")\n",
    "parser.add_option(\"-r\",\"--r\",default = \"ten\",help=\"positive negative ratio\")\n",
    "\n",
    "(opts, args) = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opts_d = 1024\n",
    "opts_n = 1\n",
    "opts_k = 512\n",
    "opts_t = \"o\"\n",
    "opts_r = \"ten\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_symmetric(a, tol=1e-8):\n",
    "    return np.allclose(a, a.T, atol=tol)\n",
    "\n",
    "def row_normalize(a_matrix, substract_self_loop):\n",
    "    if substract_self_loop == True:\n",
    "        np.fill_diagonal(a_matrix,0)\n",
    "    a_matrix = a_matrix.astype(float)\n",
    "    row_sums = a_matrix.sum(axis=1)+1e-12\n",
    "    new_matrix = a_matrix / row_sums[:, np.newaxis]\n",
    "    new_matrix[np.isnan(new_matrix) | np.isinf(new_matrix)] = 0.0\n",
    "    return new_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load network\n",
    "network_path = '' #../data/\n",
    "\n",
    "drug_drug = np.loadtxt(network_path+'mat_drug_drug.txt')\n",
    "#print 'loaded drug drug', check_symmetric(drug_drug), np.shape(drug_drug)\n",
    "true_drug = 708 # First [0:708] are drugs, the rest are compounds retrieved from ZINC15 database\n",
    "drug_chemical = np.loadtxt(network_path+'Similarity_Matrix_Drugs.txt')\n",
    "drug_chemical=drug_chemical[:true_drug,:true_drug]\n",
    "#print 'loaded drug chemical', check_symmetric(drug_chemical), np.shape(drug_chemical)\n",
    "drug_disease = np.loadtxt(network_path+'mat_drug_disease.txt')\n",
    "#print 'loaded drug disease', np.shape(drug_disease)\n",
    "drug_sideeffect = np.loadtxt(network_path+'mat_drug_se.txt')\n",
    "#print 'loaded drug sideffect', np.shape(drug_sideeffect)\n",
    "disease_drug = drug_disease.T\n",
    "sideeffect_drug = drug_sideeffect.T\n",
    "\n",
    "protein_protein = np.loadtxt(network_path+'mat_protein_protein.txt')\n",
    "#print 'loaded protein protein', check_symmetric(protein_protein), np.shape(protein_protein)\n",
    "protein_sequence = np.loadtxt(network_path+'Similarity_Matrix_Proteins.txt')\n",
    "#print 'loaded protein sequence', check_symmetric(protein_sequence), np.shape(protein_sequence)\n",
    "protein_disease = np.loadtxt(network_path+'mat_protein_disease.txt')\n",
    "#print 'loaded protein disease', np.shape(protein_disease)\n",
    "disease_protein = protein_disease.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#normalize network for mean pooling aggregation\n",
    "drug_drug_normalize = row_normalize(drug_drug,True)\n",
    "drug_chemical_normalize = row_normalize(drug_chemical,True)\n",
    "drug_disease_normalize = row_normalize(drug_disease,False)\n",
    "drug_sideeffect_normalize = row_normalize(drug_sideeffect,False)\n",
    "\n",
    "protein_protein_normalize = row_normalize(protein_protein,True)\n",
    "protein_sequence_normalize = row_normalize(protein_sequence,True)\n",
    "protein_disease_normalize = row_normalize(protein_disease,False)\n",
    "\n",
    "disease_drug_normalize = row_normalize(disease_drug,False)\n",
    "disease_protein_normalize = row_normalize(disease_protein,False)\n",
    "sideeffect_drug_normalize = row_normalize(sideeffect_drug,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define computation graph\n",
    "num_drug = len(drug_drug_normalize)\n",
    "num_protein = len(protein_protein_normalize)\n",
    "num_disease = len(disease_protein_normalize)\n",
    "num_sideeffect = len(sideeffect_drug_normalize)\n",
    "\n",
    "dim_drug = int(opts_d)\n",
    "dim_protein = int(opts_d)\n",
    "dim_disease = int(opts_d)\n",
    "dim_sideeffect = int(opts_d)\n",
    "dim_pred = int(opts_k)\n",
    "dim_pass = int(opts_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        #inputs\n",
    "        self.drug_drug = tf.placeholder(tf.float32, [num_drug, num_drug])\n",
    "        self.drug_drug_normalize = tf.placeholder(tf.float32, [num_drug, num_drug])\n",
    "\n",
    "        self.drug_chemical = tf.placeholder(tf.float32, [num_drug, num_drug])\n",
    "        self.drug_chemical_normalize = tf.placeholder(tf.float32, [num_drug, num_drug])\n",
    "\n",
    "        self.drug_disease = tf.placeholder(tf.float32, [num_drug, num_disease])\n",
    "        self.drug_disease_normalize = tf.placeholder(tf.float32, [num_drug, num_disease])\n",
    "\n",
    "        self.drug_sideeffect = tf.placeholder(tf.float32, [num_drug, num_sideeffect])\n",
    "        self.drug_sideeffect_normalize = tf.placeholder(tf.float32, [num_drug, num_sideeffect])\n",
    "\n",
    "        \n",
    "        self.protein_protein = tf.placeholder(tf.float32, [num_protein, num_protein])\n",
    "        self.protein_protein_normalize = tf.placeholder(tf.float32, [num_protein, num_protein])\n",
    "\n",
    "        self.protein_sequence = tf.placeholder(tf.float32, [num_protein, num_protein])\n",
    "        self.protein_sequence_normalize = tf.placeholder(tf.float32, [num_protein, num_protein])\n",
    "\n",
    "        self.protein_disease = tf.placeholder(tf.float32, [num_protein, num_disease])\n",
    "        self.protein_disease_normalize = tf.placeholder(tf.float32, [num_protein, num_disease])\n",
    "        \n",
    "        self.disease_drug = tf.placeholder(tf.float32, [num_disease, num_drug])\n",
    "        self.disease_drug_normalize = tf.placeholder(tf.float32, [num_disease, num_drug])\n",
    "\n",
    "        self.disease_protein = tf.placeholder(tf.float32, [num_disease, num_protein])\n",
    "        self.disease_protein_normalize = tf.placeholder(tf.float32, [num_disease, num_protein])\n",
    "\n",
    "        self.sideeffect_drug = tf.placeholder(tf.float32, [num_sideeffect, num_drug])\n",
    "        self.sideeffect_drug_normalize = tf.placeholder(tf.float32, [num_sideeffect, num_drug])\n",
    "\n",
    "        self.drug_protein = tf.placeholder(tf.float32, [num_drug, num_protein])\n",
    "        self.drug_protein_normalize = tf.placeholder(tf.float32, [num_drug, num_protein])\n",
    "\n",
    "        self.protein_drug = tf.placeholder(tf.float32, [num_protein, num_drug])\n",
    "        self.protein_drug_normalize = tf.placeholder(tf.float32, [num_protein, num_drug])\n",
    "\n",
    "        self.drug_protein_mask = tf.placeholder(tf.float32, [num_drug, num_protein])\n",
    "\n",
    "        #features\n",
    "        self.drug_embedding = weight_variable([num_drug,dim_drug])\n",
    "        self.protein_embedding = weight_variable([num_protein,dim_protein])\n",
    "        self.disease_embedding = weight_variable([num_disease,dim_disease])\n",
    "        self.sideeffect_embedding = weight_variable([num_sideeffect,dim_sideeffect])\n",
    "\n",
    "        tf.add_to_collection('l2_reg', tf.contrib.layers.l2_regularizer(1.0)(self.drug_embedding))\n",
    "        tf.add_to_collection('l2_reg', tf.contrib.layers.l2_regularizer(1.0)(self.protein_embedding))\n",
    "        tf.add_to_collection('l2_reg', tf.contrib.layers.l2_regularizer(1.0)(self.disease_embedding))\n",
    "        tf.add_to_collection('l2_reg', tf.contrib.layers.l2_regularizer(1.0)(self.sideeffect_embedding))\n",
    "\n",
    "\n",
    "\n",
    "        #feature passing weights (maybe different types of nodes can use different weights)\n",
    "        W0 = weight_variable([dim_pass+dim_drug, dim_drug])\n",
    "        b0 = bias_variable([dim_drug])\n",
    "        tf.add_to_collection('l2_reg', tf.contrib.layers.l2_regularizer(1.0)(W0))\n",
    "\n",
    "        #passing 1 times (can be easily extended to multiple passes)\n",
    "        drug_vector1 = tf.nn.l2_normalize(relu(tf.matmul(\n",
    "            tf.concat([tf.matmul(self.drug_drug_normalize, a_layer(self.drug_embedding, dim_pass)) + \\\n",
    "            tf.matmul(self.drug_chemical_normalize, a_layer(self.drug_embedding, dim_pass)) + \\\n",
    "            tf.matmul(self.drug_disease_normalize, a_layer(self.disease_embedding, dim_pass)) + \\\n",
    "            tf.matmul(self.drug_sideeffect_normalize, a_layer(self.sideeffect_embedding, dim_pass)) + \\\n",
    "            tf.matmul(self.drug_protein_normalize, a_layer(self.protein_embedding, dim_pass)), \\\n",
    "            self.drug_embedding], axis=1), W0)+b0),dim=1)\n",
    "\n",
    "        protein_vector1 = tf.nn.l2_normalize(relu(tf.matmul(\n",
    "            tf.concat([tf.matmul(self.protein_protein_normalize, a_layer(self.protein_embedding, dim_pass)) + \\\n",
    "            tf.matmul(self.protein_sequence_normalize, a_layer(self.protein_embedding, dim_pass)) + \\\n",
    "            tf.matmul(self.protein_disease_normalize, a_layer(self.disease_embedding, dim_pass)) + \\\n",
    "            tf.matmul(self.protein_drug_normalize, a_layer(self.drug_embedding, dim_pass)), \\\n",
    "            self.protein_embedding], axis=1), W0)+b0),dim=1)\n",
    "\n",
    "        disease_vector1 = tf.nn.l2_normalize(relu(tf.matmul(\n",
    "            tf.concat([tf.matmul(self.disease_drug_normalize, a_layer(self.drug_embedding, dim_pass)) + \\\n",
    "            tf.matmul(self.disease_protein_normalize, a_layer(self.protein_embedding, dim_pass)), \\\n",
    "            self.disease_embedding], axis=1), W0)+b0),dim=1)\n",
    "\n",
    "        sideeffect_vector1 = tf.nn.l2_normalize(relu(tf.matmul(\n",
    "            tf.concat([tf.matmul(self.sideeffect_drug_normalize, a_layer(self.drug_embedding, dim_pass)), \\\n",
    "            self.sideeffect_embedding], axis=1), W0)+b0),dim=1)\n",
    "\n",
    "\n",
    "        self.drug_representation = drug_vector1\n",
    "        self.protein_representation = protein_vector1\n",
    "        self.disease_representation = disease_vector1\n",
    "        self.sideeffect_representation = sideeffect_vector1\n",
    "\n",
    "        #reconstructing networks\n",
    "        self.drug_drug_reconstruct = bi_layer(self.drug_representation,self.drug_representation, sym=True, dim_pred=dim_pred)\n",
    "        self.drug_drug_reconstruct_loss = tf.reduce_sum(tf.multiply((self.drug_drug_reconstruct-self.drug_drug), (self.drug_drug_reconstruct-self.drug_drug)))\n",
    "\n",
    "        self.drug_chemical_reconstruct = bi_layer(self.drug_representation,self.drug_representation, sym=True, dim_pred=dim_pred)\n",
    "        self.drug_chemical_reconstruct_loss = tf.reduce_sum(tf.multiply((self.drug_chemical_reconstruct-self.drug_chemical), (self.drug_chemical_reconstruct-self.drug_chemical)))\n",
    "\n",
    "\n",
    "        self.drug_disease_reconstruct = bi_layer(self.drug_representation,self.disease_representation, sym=False, dim_pred=dim_pred)\n",
    "        self.drug_disease_reconstruct_loss = tf.reduce_sum(tf.multiply((self.drug_disease_reconstruct-self.drug_disease), (self.drug_disease_reconstruct-self.drug_disease)))\n",
    "\n",
    "\n",
    "        self.drug_sideeffect_reconstruct = bi_layer(self.drug_representation,self.sideeffect_representation, sym=False, dim_pred=dim_pred)\n",
    "        self.drug_sideeffect_reconstruct_loss = tf.reduce_sum(tf.multiply((self.drug_sideeffect_reconstruct-self.drug_sideeffect), (self.drug_sideeffect_reconstruct-self.drug_sideeffect)))\n",
    "\n",
    "\n",
    "        self.protein_protein_reconstruct = bi_layer(self.protein_representation,self.protein_representation, sym=True, dim_pred=dim_pred)\n",
    "        self.protein_protein_reconstruct_loss = tf.reduce_sum(tf.multiply((self.protein_protein_reconstruct-self.protein_protein), (self.protein_protein_reconstruct-self.protein_protein)))\n",
    "\n",
    "        self.protein_sequence_reconstruct = bi_layer(self.protein_representation,self.protein_representation, sym=True, dim_pred=dim_pred)\n",
    "        self.protein_sequence_reconstruct_loss = tf.reduce_sum(tf.multiply((self.protein_sequence_reconstruct-self.protein_sequence), (self.protein_sequence_reconstruct-self.protein_sequence)))\n",
    "\n",
    "\n",
    "        self.protein_disease_reconstruct = bi_layer(self.protein_representation,self.disease_representation, sym=False, dim_pred=dim_pred)\n",
    "        self.protein_disease_reconstruct_loss = tf.reduce_sum(tf.multiply((self.protein_disease_reconstruct-self.protein_disease), (self.protein_disease_reconstruct-self.protein_disease)))\n",
    "\n",
    "\n",
    "        self.drug_protein_reconstruct = bi_layer(self.drug_representation,self.protein_representation, sym=False, dim_pred=dim_pred)\n",
    "        tmp = tf.multiply(self.drug_protein_mask, (self.drug_protein_reconstruct-self.drug_protein))\n",
    "        self.drug_protein_reconstruct_loss = tf.reduce_sum(tf.multiply(tmp, tmp))\n",
    "\n",
    "        self.l2_loss = tf.add_n(tf.get_collection(\"l2_reg\"))\n",
    "\n",
    "        self.loss = self.drug_protein_reconstruct_loss + 1.0*(self.drug_drug_reconstruct_loss+self.drug_chemical_reconstruct_loss+\n",
    "                                                            self.drug_disease_reconstruct_loss+self.drug_sideeffect_reconstruct_loss+\n",
    "                                                            self.protein_protein_reconstruct_loss+self.protein_sequence_reconstruct_loss+\n",
    "                                                            self.protein_disease_reconstruct_loss) + self.l2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()\n",
    "with graph.as_default():\n",
    "    model = Model()\n",
    "    learning_rate = tf.placeholder(tf.float32, [])\n",
    "    total_loss = model.loss\n",
    "    dti_loss = model.drug_protein_reconstruct_loss\n",
    "\n",
    "    optimize = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients, variables = zip(*optimize.compute_gradients(total_loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, int(opts_n))\n",
    "    optimizer = optimize.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    eval_pred = model.drug_protein_reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(DTItrain, DTIvalid, DTItest, graph, verbose=True, num_steps = 4000):\n",
    "    drug_protein = np.zeros((num_drug,num_protein))\n",
    "    mask = np.zeros((num_drug,num_protein))\n",
    "    for ele in DTItrain:\n",
    "        drug_protein[ele[0],ele[1]] = ele[2]\n",
    "        mask[ele[0],ele[1]] = 1\n",
    "    protein_drug = drug_protein.T\n",
    "\n",
    "    drug_protein_normalize = row_normalize(drug_protein,False)\n",
    "    protein_drug_normalize = row_normalize(protein_drug,False)\n",
    "\n",
    "    lr = 0.001\n",
    "\n",
    "    best_valid_aupr = 0\n",
    "    best_valid_auc = 0\n",
    "    test_aupr = 0\n",
    "    test_auc = 0\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        for i in range(num_steps):\n",
    "            _, tloss, dtiloss, results = sess.run([optimizer,total_loss,dti_loss,eval_pred], \\\n",
    "                                        feed_dict={model.drug_drug:drug_drug, model.drug_drug_normalize:drug_drug_normalize,\\\n",
    "                                        model.drug_chemical:drug_chemical, model.drug_chemical_normalize:drug_chemical_normalize,\\\n",
    "                                        model.drug_disease:drug_disease, model.drug_disease_normalize:drug_disease_normalize,\\\n",
    "                                        model.drug_sideeffect:drug_sideeffect, model.drug_sideeffect_normalize:drug_sideeffect_normalize,\\\n",
    "                                        model.protein_protein:protein_protein, model.protein_protein_normalize:protein_protein_normalize,\\\n",
    "                                        model.protein_sequence:protein_sequence, model.protein_sequence_normalize:protein_sequence_normalize,\\\n",
    "                                        model.protein_disease:protein_disease, model.protein_disease_normalize:protein_disease_normalize,\\\n",
    "                                        model.disease_drug:disease_drug, model.disease_drug_normalize:disease_drug_normalize,\\\n",
    "                                        model.disease_protein:disease_protein, model.disease_protein_normalize:disease_protein_normalize,\\\n",
    "                                        model.sideeffect_drug:sideeffect_drug, model.sideeffect_drug_normalize:sideeffect_drug_normalize,\\\n",
    "                                        model.drug_protein:drug_protein, model.drug_protein_normalize:drug_protein_normalize,\\\n",
    "                                        model.protein_drug:protein_drug, model.protein_drug_normalize:protein_drug_normalize,\\\n",
    "                                        model.drug_protein_mask:mask,\\\n",
    "                                        learning_rate: lr})\n",
    "            #every 25 steps of gradient descent, evaluate the performance, other choices of this number are possible\n",
    "            if i % 25 == 0 and verbose == True:\n",
    "                print 'step',i,'total and dtiloss',tloss, dtiloss\n",
    "\n",
    "                pred_list = []\n",
    "                ground_truth = []\n",
    "                for ele in DTIvalid:\n",
    "                    pred_list.append(results[ele[0],ele[1]])\n",
    "                    ground_truth.append(ele[2])\n",
    "                valid_auc = roc_auc_score(ground_truth, pred_list)\n",
    "                valid_aupr = average_precision_score(ground_truth, pred_list)\n",
    "                if valid_aupr >= best_valid_aupr:\n",
    "                    best_valid_aupr = valid_aupr\n",
    "                    best_valid_auc = valid_auc\n",
    "                    pred_list = []\n",
    "                    ground_truth = []\n",
    "                    for ele in DTItest:\n",
    "                        pred_list.append(results[ele[0],ele[1]])\n",
    "                        ground_truth.append(ele[2])\n",
    "                    test_auc = roc_auc_score(ground_truth, pred_list)\n",
    "                    test_aupr = average_precision_score(ground_truth, pred_list)\n",
    "                print 'valid auc aupr,', valid_auc, valid_aupr, 'test auc aupr', test_auc, test_aupr\n",
    "    return best_valid_auc, best_valid_aupr, test_auc, test_aupr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample round 1\n",
      "WARNING:tensorflow:From <ipython-input-9-f3d899884898>:20: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "step 0 total and dtiloss 2.25801e+08 1630.73\n",
      "valid auc aupr, 0.517269414073"
     ]
    }
   ],
   "source": [
    "test_auc_round = []\n",
    "test_aupr_round = []\n",
    "for r in xrange(10):\n",
    "    print 'sample round',r+1\n",
    "    if opts_t == 'o':\n",
    "        dti_o = np.loadtxt(network_path+'mat_drug_protein.txt')\n",
    "    else:\n",
    "        dti_o = np.loadtxt(network_path+'mat_drug_protein_'+opts_t+'.txt')\n",
    "\n",
    "    whole_positive_index = []\n",
    "    whole_negative_index = []\n",
    "    for i in xrange(np.shape(dti_o)[0]):\n",
    "        for j in xrange(np.shape(dti_o)[1]):\n",
    "            if int(dti_o[i][j]) == 1:\n",
    "                whole_positive_index.append([i,j])\n",
    "            elif int(dti_o[i][j]) == 0:\n",
    "                whole_negative_index.append([i,j])\n",
    "\n",
    "\n",
    "    if opts_r == 'ten':\n",
    "        negative_sample_index = np.random.choice(np.arange(len(whole_negative_index)),size=10*len(whole_positive_index),replace=False)\n",
    "    elif opts_r == 'all':\n",
    "        negative_sample_index = np.random.choice(np.arange(len(whole_negative_index)),size=len(whole_negative_index),replace=False)\n",
    "    else:\n",
    "        print 'wrong positive negative ratio'\n",
    "        break\n",
    "\n",
    "    data_set = np.zeros((len(negative_sample_index)+len(whole_positive_index),3),dtype=int)\n",
    "    count = 0\n",
    "    for i in whole_positive_index:\n",
    "        data_set[count][0] = i[0]\n",
    "        data_set[count][1] = i[1]\n",
    "        data_set[count][2] = 1\n",
    "        count += 1\n",
    "    for i in negative_sample_index:\n",
    "        data_set[count][0] = whole_negative_index[i][0]\n",
    "        data_set[count][1] = whole_negative_index[i][1]\n",
    "        data_set[count][2] = 0\n",
    "        count += 1\n",
    "\n",
    "\n",
    "\n",
    "    if opts_t == 'unique':\n",
    "        whole_positive_index_test = []\n",
    "        whole_negative_index_test = []\n",
    "        for i in xrange(np.shape(dti_o)[0]):\n",
    "            for j in xrange(np.shape(dti_o)[1]):\n",
    "                if int(dti_o[i][j]) == 3:\n",
    "                    whole_positive_index_test.append([i,j])\n",
    "                elif int(dti_o[i][j]) == 2:\n",
    "                    whole_negative_index_test.append([i,j])\n",
    "\n",
    "        if opts_r == 'ten':\n",
    "            negative_sample_index_test = np.random.choice(np.arange(len(whole_negative_index_test)),size=10*len(whole_positive_index_test),replace=False)\n",
    "        elif opts_r == 'all':\n",
    "            negative_sample_index_test = np.random.choice(np.arange(len(whole_negative_index_test)),size=whole_negative_index_test,replace=False)\n",
    "        else:\n",
    "            print 'wrong positive negative ratio'\n",
    "            break\n",
    "        data_set_test = np.zeros((len(negative_sample_index_test)+len(whole_positive_index_test),3),dtype=int)\n",
    "        count = 0\n",
    "        for i in whole_positive_index_test:\n",
    "            data_set_test[count][0] = i[0]\n",
    "            data_set_test[count][1] = i[1]\n",
    "            data_set_test[count][2] = 1\n",
    "            count += 1\n",
    "        for i in negative_sample_index_test:\n",
    "            data_set_test[count][0] = whole_negative_index_test[i][0]\n",
    "            data_set_test[count][1] = whole_negative_index_test[i][1]\n",
    "            data_set_test[count][2] = 0\n",
    "            count += 1\n",
    "\n",
    "        DTItrain = data_set\n",
    "        DTItest = data_set_test\n",
    "        rs = np.random.randint(0,1000,1)[0]\n",
    "        DTItrain, DTIvalid =  train_test_split(DTItrain, test_size=0.05, random_state=rs)\n",
    "        v_auc, v_aupr, t_auc, t_aupr = train_and_evaluate(DTItrain=DTItrain, DTIvalid=DTIvalid, DTItest=DTItest, graph=graph, num_steps=3000)\n",
    "\n",
    "        test_auc_round.append(t_auc)\n",
    "        test_aupr_round.append(t_aupr)\n",
    "        np.savetxt('test_auc', test_auc_round)\n",
    "        np.savetxt('test_aupr', test_aupr_round)\n",
    "\n",
    "    else:\n",
    "        test_auc_fold = []\n",
    "        test_aupr_fold = []\n",
    "        rs = np.random.randint(0,1000,1)[0]\n",
    "        kf = StratifiedKFold(data_set[:,2], n_folds=10, shuffle=True, random_state=rs)\n",
    "\n",
    "        for train_index, test_index in kf:\n",
    "            DTItrain, DTItest = data_set[train_index], data_set[test_index]\n",
    "            DTItrain, DTIvalid =  train_test_split(DTItrain, test_size=0.05, random_state=rs)\n",
    "\n",
    "            v_auc, v_aupr, t_auc, t_aupr = train_and_evaluate(DTItrain=DTItrain, DTIvalid=DTIvalid, DTItest=DTItest, graph=graph, num_steps=3000)\n",
    "            test_auc_fold.append(t_auc)\n",
    "            test_aupr_fold.append(t_aupr)\n",
    "\n",
    "        test_auc_round.append(np.mean(test_auc_fold))\n",
    "        test_aupr_round.append(np.mean(test_aupr_fold))\n",
    "        np.savetxt('test_auc', test_auc_round)\n",
    "        np.savetxt('test_aupr', test_aupr_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
